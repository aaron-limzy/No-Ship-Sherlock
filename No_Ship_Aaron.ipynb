{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "import cufflinks as cf\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import os\n",
    "#pip install chart_studio\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Google Drive\\\\Masters\\\\Sem 3\\\\Python_Stuff\\\\Aaron [No Ship]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = r\"D:\\Google Drive\\Masters\\Sem 3\\EBAC No Ship, Sherlock\\Capstone\\raw data\"\n",
    "os.chdir(full_path)\n",
    "#os.chdir(r\"C:\\Users\\Admin\\Google Drive\\Masters\\Sem 3\\EBAC No Ship, Sherlock\\Capstone\\raw data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Google Drive\\\\Masters\\\\Sem 3\\\\EBAC No Ship, Sherlock\\\\Capstone\\\\raw data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Masterdata of shipment routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to get the shipment data\n",
    "excel_route_file = \"APJ Route TAT Master File V20210129.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shipment_routes(excel_route_file):\n",
    "    xls = pd.ExcelFile(excel_route_file)\n",
    "    df_ship_routes = pd.read_excel(xls,\"MasterData\" )\n",
    "    xls = [] # Free up resource\n",
    "    print(\"Length of Excel: {}\".format(len(df_ship_routes)))\n",
    "    df_ship_routes.head()\n",
    "    # Check that there are no duplicates\n",
    "    sum(df_ship_routes['Route'].duplicated())\n",
    "    #df_ship_routes = df_ship_routes['Mode'].str.lower() == \"air\"\n",
    "    print(\"The count of mode by air: {}\".format(sum(df_ship_routes['Mode'].str.lower() == \"ocean\")))\n",
    "    return df_ship_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Excel: 11958\n",
      "The count of mode by air: 0\n"
     ]
    }
   ],
   "source": [
    "df_ship_routes = get_shipment_routes(excel_route_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11958"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ship_routes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Shipment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SearchResult_1615300381378_Jan21.csv', 'SearchResult_1615301179189_Dec20.csv', 'SearchResult_1615301591520_Nov20.csv', 'SearchResult_1615301861292_Oct20.csv', 'SearchResult_1615302057067_Sep20.csv', 'SearchResult_1615302586308_Aug20.csv', 'SearchResult_1615302848713_Jul20.csv', 'SearchResult_1615303098163_Jun20.csv', 'SearchResult_1615303294492_May20.csv', 'SearchResult_1615303594160_Apr20.csv', 'SearchResult_1615303885072_Mar20.csv', 'SearchResult_1615304080226_Feb20.csv']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\".\\monthly\")\n",
    "file_names = [f for f in os.listdir() if f.find(\"SearchResult\")>=0 ]\n",
    "\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_shipment_data(excel_ship_file):\n",
    "    print(\"Excel File Name: {}\".format(excel_ship_file))\n",
    "    df_ship_data = pd.read_csv(excel_ship_file)\n",
    "    print(\"File shape is: {}\".format(df_ship_data.shape))\n",
    "    return df_ship_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Takes a df and 2 of it's columns. If the original is null, replaces it with the replacement. \n",
    "def replace_original_if_null(df, original, replacement):\n",
    "    df[original] = df.apply(\n",
    "        lambda row: row[replacement] if pd.isnull(row[original]) else row[original],\n",
    "        axis=1)\n",
    "    return df\n",
    "\n",
    "def wrongyear_data_cleaning(df):\n",
    "        # ---------------- Correction of timing for 2020/2021/2022 -------------------------\n",
    "    \n",
    "    #Change Arrived at destination port year from 2020 to 2021 when ATA time - ATA received time < -300 \n",
    "    df['Arrived at Destination Port Milestone Time'] = df['Arrived at Destination Port Milestone Time'].mask(\n",
    "        (df['Arrived at Destination Port Milestone Time']- \\\n",
    "                         df['Arrived at Destination Port Milestone Received Time']).dt.days < -300, \n",
    "        df['Arrived at Destination Port Milestone Time'] +  pd.to_timedelta(366, unit='D'))\n",
    "    \n",
    "    \n",
    "    df['Arrived at Destination Port Milestone Local Time'] = df['Arrived at Destination Port Milestone Local Time'].mask(\n",
    "        (df['Arrived at Destination Port Milestone Local Time'] - \\\n",
    "             df['Arrived at Destination Port Milestone Time']).dt.days < -300,\n",
    "            df['Arrived at Destination Port Milestone Local Time'] +  pd.to_timedelta(366, unit='D'))\n",
    "    \n",
    "    #Change Arrived at destination port year from 2022 to 2021 when ATA time - ATA received time > 300 \n",
    "    df['Arrived at Destination Port Milestone Time'] = df['Arrived at Destination Port Milestone Time'].mask(\n",
    "        (df['Arrived at Destination Port Milestone Time']- \\\n",
    "                         df['Arrived at Destination Port Milestone Received Time']).dt.days >= 300, \n",
    "        df['Arrived at Destination Port Milestone Time'] -  pd.to_timedelta(366, unit='D'))\n",
    "    \n",
    "    \n",
    "    df['Arrived at Destination Port Milestone Local Time'] = df['Arrived at Destination Port Milestone Local Time'].mask(\n",
    "        (df['Arrived at Destination Port Milestone Local Time'] - \\\n",
    "         df['Arrived at Destination Port Milestone Time']).dt.days > 300, \n",
    "        df['Arrived at Destination Port Milestone Local Time'] -  pd.to_timedelta(366, unit='D'))\n",
    "\n",
    "\n",
    "\n",
    "    #Change Departed from Origin Port Milestone year from 2020 to 2021 when ATD time - ATD received time < -300 \n",
    "    df['Departed from Origin Port Milestone Time'] = df['Departed from Origin Port Milestone Time'].mask(\n",
    "         (df['Arrived at Destination Port Milestone Local Time'] - \\\n",
    "         df['Arrived at Destination Port Milestone Time']).dt.days < -300, \n",
    "        df['Departed from Origin Port Milestone Time'] +  pd.to_timedelta(366, unit='D'))\n",
    "    \n",
    "    df['Departed from Origin Port Milestone Local Time'] = df['Departed from Origin Port Milestone Local Time'].mask(\n",
    "        (df['Departed from Origin Port Milestone Local Time'] - \\\n",
    "         df['Departed from Origin Port Milestone Time']).dt.days < -300, \n",
    "        df['Departed from Origin Port Milestone Local Time'] +  pd.to_timedelta(366, unit='D'))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def data_cleaning(df):\n",
    "    \n",
    "    # First, Want to clean up data that is obviously duplicated. \n",
    "    print(\"Data that is duplicated exactly: {}\".format(sum(df.duplicated())))\n",
    "    df = df[~df.duplicated()]\n",
    "\n",
    "    # If H-K missing, replace with Latest D-G\n",
    "    #[(Replacement, original)]\n",
    "    replace_column = [('Onboard at Origin Port Milestone Local Time','Departed from Origin Port Milestone Local Time'),\n",
    "       ('Onboard at Origin Port Milestone Time','Departed from Origin Port Milestone Time'),\n",
    "       ('Onboard at Origin Port Milestone Received Time','Departed from Origin Port Milestone Received Time'),\n",
    "       ('Onboard at Origin Port Milestone Source', 'Departed from Origin Port Milestone Source')]\n",
    "    \n",
    "    for (r,o) in replace_column:\n",
    "        df =  replace_original_if_null(df=df, original=o, replacement=r)\n",
    "        \n",
    "    \n",
    "    # Cast to datetime. \n",
    "    col_to_datetime = ['Departed from Origin Port Milestone Local Time',\n",
    "       'Departed from Origin Port Milestone Time',\n",
    "       'Departed from Origin Port Milestone Received Time',\n",
    "       'Arrived at Destination Port Milestone Local Time',\n",
    "       'Arrived at Destination Port Milestone Time',\n",
    "       'Arrived at Destination Port Milestone Received Time',\n",
    "       'Est. Arrival at Destination Port Milestone Local Time',\n",
    "       'Est. Arrival at Destination Port Milestone Time',\n",
    "       'Est. Arrival at Destination Port Milestone Received Time']\n",
    "\n",
    "    for col in col_to_datetime:\n",
    "        #df = df[df[c].notnull()] # Want to get rid of all the null values. \n",
    "        \n",
    "        print(\"Changing Column '{}' to datetime.\".format(col))\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        #df[c] = df[c].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S') if not pd.isnull(x) else x)\n",
    "        \n",
    "    \n",
    "    # Want to do the year clean up for those data with wrongly labelled years. \n",
    "    df = wrongyear_data_cleaning(df)\n",
    "        \n",
    "    3# M later than I\n",
    "    #\"Arrived at Destination Port Milestone Time\" > \"Departed from Origin Port Milestone Time\"\n",
    "    df = df[df[\"Arrived at Destination Port Milestone Time\"] > df[\"Departed from Origin Port Milestone Time\"]]\n",
    "\n",
    "    ## R Before M & N\n",
    "    #\"Est. Arrival at Destination Port Milestone Received Time\" < \"Arrived at Destination Port Milestone Time\"\n",
    "    df = df[df[\"Est. Arrival at Destination Port Milestone Received Time\"] < df[\"Arrived at Destination Port Milestone Time\"]]\n",
    "\n",
    "    #\"Est. Arrival at Destination Port Milestone Received Time\" < \"Arrived at Destination Port Milestone Received Time\"\n",
    "    df=df[df[\"Est. Arrival at Destination Port Milestone Received Time\"] < df[\"Arrived at Destination Port Milestone Received Time\"]]\n",
    "\n",
    "     ## R Later than I\n",
    "     #\"Est. Arrival at Destination Port Milestone Received Time\" > \"Departed from Origin Port Milestone Time\"\n",
    "    df=df[df[\"Est. Arrival at Destination Port Milestone Received Time\"] < df[\"Departed from Origin Port Milestone Time\"]]\n",
    "\n",
    "     # Sort by Latest M, Take earliest R\n",
    "     # \"Arrived at Destination Port Milestone Time\"\n",
    "     # \"Est. Arrival at Destination Port Milestone Received Time\"\n",
    "    df.sort_values([\"Arrived at Destination Port Milestone Time\", \"Est. Arrival at Destination Port Milestone Received Time\"], ascending=[False, True], inplace=True)\n",
    "\n",
    "    # Drop the duplicated Shipment IDs.\n",
    "    df.drop_duplicates(subset=['Shipment id'], keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel File Name: SearchResult_1615300381378_Jan21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\risk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning:\n",
      "\n",
      "Columns (21,22,23,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File shape is: (164584, 33)\n",
      "Excel File Name: SearchResult_1615301179189_Dec20.csv\n",
      "File shape is: (148071, 33)\n",
      "Excel File Name: SearchResult_1615301591520_Nov20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\risk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning:\n",
      "\n",
      "Columns (21,23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File shape is: (105069, 33)\n",
      "Excel File Name: SearchResult_1615301861292_Oct20.csv\n",
      "File shape is: (124359, 33)\n",
      "Excel File Name: SearchResult_1615302057067_Sep20.csv\n",
      "File shape is: (600000, 33)\n",
      "Excel File Name: SearchResult_1615302586308_Aug20.csv\n",
      "File shape is: (600000, 33)\n",
      "Excel File Name: SearchResult_1615302848713_Jul20.csv\n",
      "File shape is: (600000, 33)\n",
      "Excel File Name: SearchResult_1615303098163_Jun20.csv\n",
      "File shape is: (119388, 33)\n",
      "Excel File Name: SearchResult_1615303294492_May20.csv\n",
      "File shape is: (600000, 33)\n",
      "Excel File Name: SearchResult_1615303594160_Apr20.csv\n",
      "File shape is: (600000, 33)\n",
      "Excel File Name: SearchResult_1615303885072_Mar20.csv\n",
      "File shape is: (276524, 33)\n",
      "Excel File Name: SearchResult_1615304080226_Feb20.csv\n",
      "File shape is: (105049, 33)\n"
     ]
    }
   ],
   "source": [
    "# To read all the excel sheets. \n",
    "df_unique_shipment_all = pd.DataFrame()\n",
    "for f in file_names:\n",
    "    df_unique_shipment_all=df_unique_shipment_all.append(get_shipment_data(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data that is duplicated exactly: 2121838\n",
      "Changing Column 'Departed from Origin Port Milestone Local Time' to datetime.\n",
      "Changing Column 'Departed from Origin Port Milestone Time' to datetime.\n",
      "Changing Column 'Departed from Origin Port Milestone Received Time' to datetime.\n",
      "Changing Column 'Arrived at Destination Port Milestone Local Time' to datetime.\n",
      "Changing Column 'Arrived at Destination Port Milestone Time' to datetime.\n",
      "Changing Column 'Arrived at Destination Port Milestone Received Time' to datetime.\n",
      "Changing Column 'Est. Arrival at Destination Port Milestone Local Time' to datetime.\n",
      "Changing Column 'Est. Arrival at Destination Port Milestone Time' to datetime.\n",
      "Changing Column 'Est. Arrival at Destination Port Milestone Received Time' to datetime.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "keep must be either \"first\", \"last\" or False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-bc9730138958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_unique_shipment_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_unique_shipment_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-01eb2c5ea5e5>\u001b[0m in \u001b[0;36mdata_cleaning\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# Drop the duplicated Shipment IDs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Shipment id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'First'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\risk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5111\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5112\u001b[1;33m         \u001b[0mduplicated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5114\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\risk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   5252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5253\u001b[0m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_group_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxnull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mduplicated_int64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5256\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_func_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.duplicated_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: keep must be either \"first\", \"last\" or False"
     ]
    }
   ],
   "source": [
    "df_unique_shipment_original = data_cleaning(df_unique_shipment_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment = df_unique_shipment_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unique_shipment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unique_shipment[\"Shipment id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want the First ones, since we have sorted it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_unique_shipment=df_unique_shipment[~ df_unique_shipment[\"Shipment id\"].duplicated(keep=\"first\")]\n",
    "df_unique_shipment.drop_duplicates(subset =\"Shipment id\", keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unique_shipment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Might want to looking into correcting data entry errors with the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df_unique_shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_diff = abs(df[\"Arrived at Destination Port Milestone Time\"] - df[\"Departed from Origin Port Milestone Time\"])\n",
    "# time_diff > pd.Timedelta(\"300 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[(df[\"Est. Arrival at Destination Port Milestone Received Time\"] - df[\"Arrived at Destination Port Milestone Time\"]) > pd.Timedelta(\"300 days\")][[\"Est. Arrival at Destination Port Milestone Received Time\",\"Arrived at Destination Port Milestone Time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unique_shipment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment[\"Delay Est/Arrival Duration\"] = df_unique_shipment[\"Arrived at Destination Port Milestone Time\"] - df_unique_shipment[\"Est. Arrival at Destination Port Milestone Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to do a little clean up here as there are ships that are earlier by 100 days\n",
    "df_unique_shipment=df_unique_shipment[df_unique_shipment[\"Delay Est/Arrival Duration\"] > -pd.Timedelta(\"100 days\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment[df_unique_shipment[\"Delay Est/Arrival Duration\"]<pd.Timedelta(\"0 days\")].sort_values(\"Delay Est/Arrival Duration\")[\"Delay Est/Arrival Duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unique_shipment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment[df_unique_shipment[\"Delay Est/Arrival Duration\"] < pd.Timedelta(\"0 days\")].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment.fillna({\"Origin Ctry\":\"-\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ship_data[df_ship_data.duplicated(subset=[\"Shipment id\"])][\"Shipment id\"].value_counts().nlargest(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ship_data[df_ship_data[\"Shipment id\"] == 39640134][~ df_ship_data[df_ship_data[\"Shipment id\"] == 39640134].duplicated()].to_excel(\"duplicate.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [15, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_unique_shipment[\"Route\"].value_counts().iplot(kind='bar')\n",
    "#df_unique_shipment[\"Route\"].value_counts().nlargest(50).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.histogram(df_unique_shipment[\"Route\"]).update_xaxes(categoryorder=\"total descending\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to get a list of unique shipment\n",
    "#unique_shipment_ID = df_ship_data[\"Shipment id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to get the columns that are at least 80% Filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(df, col_filled=80):\n",
    "\n",
    "    # Want to get the column with the most missing data\n",
    "    missing_value_series = df.isnull().sum() / df.shape[0] * 100.00\n",
    "    # Sort the missing %\n",
    "    missing_value_series.sort_values(ascending=False, inplace=True)\n",
    "    #missing_value_series\n",
    "    col_dict=missing_value_series.to_dict()\n",
    "    col = [k for k, d in col_dict.items() if d < (100 - col_filled)]\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_shipment[\"Route\"] = df_unique_shipment[\"Route\"].astype(\"str\")\n",
    "df_ship_routes['Route']  = df_ship_routes['Route'].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the data is of string type\n",
    "#type(df_unique_shipment['Route'])\n",
    "# type(df_ship_routes['Route'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_unique_shipment.merge(df_ship_routes, on=\"Route\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_combined.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=df_combined[get_col_names(df_combined)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_combined[\"Destination Ctry\"], title='Histogram of Destination Ctry').update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_combined[\"Origin Ctry\"], title='Histogram of Origin Ctry').update_xaxes(categoryorder=\"total descending\", title=\"Origin Country\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[df_combined[\"Description\"].isnull()][\"Route\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[[\"Delay Est/Arrival Duration\", \"Arrived at Destination Port Milestone Time\", \"Est. Arrival at Destination Port Milestone Time\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined.iloc[5][[\"Delay Est/Arrival Duration\", \"Arrived at Destination Port Milestone Time\", \"Est. Arrival at Destination Port Milestone Time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"Delay Est/Arrival Duration (Hour)\"] = df_combined[\"Delay Est/Arrival Duration\"].apply(lambda x: x.total_seconds()/(60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"Delay Est/Arrival Duration (Hour)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_combined[df_combined[\"Origin Ctry\"].notnull()][[\"Delay Est/Arrival Duration (Hour)\", \"Origin Ctry\"]], title='Delay between estimates', color=\"Origin Ctry\").update_xaxes(categoryorder=\"total descending\", title=\"Time (Hours)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\Google Drive\\Masters\\Sem 3\\Python_Stuff\\Aaron [No Ship]\")\n",
    "df_combined.to_csv(\"Cleaned_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Port of LoadingDestination CtryMode\" in df_combined.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
